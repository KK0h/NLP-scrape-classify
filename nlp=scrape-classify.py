# -*- coding: utf-8 -*-
"""Web scraping 2

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1n6zKoKDPmWr12kcTppqWJgKNcwOs5wUU
"""

!pip install scrapy

import scrapy
url = 'https://jesscscott.wordpress.com/'

urls = []
collection = []
class MySpider2(scrapy.Spider):
  name='twolevels'
  def start_requests( self ):
    yield scrapy.Request( url=url, callback=self.parse )

  def parse( self, response ):
    weblinks = response.xpath('//a[@class="wp-block-latest-posts__post-title"]/@href').extract()
    weblink_text = response.css('p::text').extract()
    collection.append( weblink_text )
    for url in weblinks:
      urls.append( url )
      yield response.follow( url=url, callback=self.parse_linktext )
    return url
    return collection

  def parse_linktext( self, response ):
    weblink_text = response.css('p::text').extract()
    print( weblink_text )
    collection.append( weblink_text )
    return collection

from scrapy.crawler import CrawlerProcess

process = CrawlerProcess()
process.crawl(MySpider2)
process.start()

import pandas as pd
import numpy as np
pd.options.display.max_colwidth = 100
import string

url = pd.Series(urls)
text = pd.Series(collection)
df = pd.DataFrame({'url':url, 'text':text})
print(df)

df.shape

import nltk
nltk.download('punkt')
nltk.download('wordnet')
from nltk import sent_tokenize
from nltk import word_tokenize
from nltk import wordpunct_tokenize
nltk.download('stopwords')
from nltk.corpus import stopwords
stop_words = set(stopwords.words('english'))
from nltk.stem import WordNetLemmatizer
lemmer = WordNetLemmatizer()

!pip install contractions
import contractions

!pip install emot
import emot

emot_obj = emot.core.emot()

def convert_emoticons(text):
  emoticons_dict = emot_obj.emoticons(text)
  if emoticons_dict['flag']==True:
    for i in range(len(emoticons_dict['value'])):
      pattern = re.compile(re.escape(emoticons_dict['value'][i]))
      text = pattern.sub(" "+emoticons_dict['mean'][i]+" ", text)
    text = text.strip()
  return text

from emot.emo_unicode import UNICODE_EMOJI
import re

replacement_dict = dict((re.escape(k), v.replace(':', ' ').replace('_',' ')) for k, v in UNICODE_EMOJI.items())
pattern = re.compile("|".join(replacement_dict.keys()))
def convert_emojis(text):
  text = pattern.sub(lambda m: replacement_dict[re.escape(m.group(0))], text)
  text = text.strip()
  return text

df['text_cleaned'] = df['text'].apply(lambda x: ' '.join(x))
df['text_cleaned'] = df['text_cleaned'].apply(lambda x: contractions.fix(x))
df['text_cleaned'] = df['text_cleaned'].apply(lambda x: x.lower().translate(
    dict((ord(punct), None) for punct in string.punctuation)
))
df['text_cleaned'] = df['text_cleaned'].apply(lambda x: convert_emoticons(x))
df['text_cleaned'] = df['text_cleaned'].apply(lambda x: convert_emojis(x))
df['text_cleaned']= df["text_cleaned"].str.replace(r'\d+', '', regex = True)
df['text_cleaned'] = df["text_cleaned"].str.replace(r'\b[a-zA-Z]\b', '', regex = True)

df['text_cleaned'] = df['text_cleaned'].apply(lambda x: word_tokenize(x))
df['text_cleaned'] = df['text_cleaned'].apply(lambda x: [lemmer.lemmatize(t) for t in x if t not in stop_words])
df['text_cleaned'] = df['text_cleaned'].apply(lambda x: " ".join(x))

print(df[['text_cleaned']])

!pip install transformers

from transformers import pipeline

classifier = pipeline("zero-shot-classification")
df['output'] = df['text_cleaned'].apply(lambda x: classifier(x, candidate_labels=['education', 'politics', 'sports'])['labels'][0])
df

